"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[588],{1989:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"deployment","title":"Deployment: The Bridge to Value","description":"Having established the value these models can provide, it\u2019s equally important to recognize that they must be deployed or put into production in order to realize that value. There are various ways to deploy a model, but a common method involves wrapping it in a service that exposes one or more endpoints that can receive input (e.g., text) and return the model\u2019s predictions. This approach allows the model\u2019s functionality to be integrated seamlessly into various applications, including customer-facing web frontends, internal analytics software, and automated data processing pipelines.","source":"@site/docs/deployment.md","sourceDirName":".","slug":"/deployment","permalink":"/docs/deployment","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"deployment","sidebar_label":"3. Deployment - The Bridge to Value","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"2. Background","permalink":"/docs/background"},"next":{"title":"4. Architecture","permalink":"/docs/architecture"}}');var s=i(4848),r=i(8453);const a={id:"deployment",sidebar_label:"3. Deployment - The Bridge to Value",sidebar_position:3},o="Deployment: The Bridge to Value",l={},c=[{value:"The Deployment Bottleneck",id:"the-deployment-bottleneck",level:2},{value:"Roles and Responsibilities",id:"roles-and-responsibilities",level:2},{value:"Deployment Strategies",id:"deployment-strategies",level:2},{value:"Real-Time Deployment",id:"real-time-deployment",level:3},{value:"Batch Deployment",id:"batch-deployment",level:3},{value:"Existing Deployment Solutions",id:"existing-deployment-solutions",level:2},{value:"Managed Platforms",id:"managed-platforms",level:3},{value:"Open-Source Model-Serving Frameworks",id:"open-source-model-serving-frameworks",level:3},{value:"DIY Solutions",id:"diy-solutions",level:3},{value:"Current Deployment Gap",id:"current-deployment-gap",level:2}];function d(e){const n={a:"a",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",section:"section",strong:"strong",sup:"sup",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"deployment-the-bridge-to-value",children:"Deployment: The Bridge to Value"})}),"\n",(0,s.jsx)(n.p,{children:"Having established the value these models can provide, it\u2019s equally important to recognize that they must be deployed or put into production in order to realize that value. There are various ways to deploy a model, but a common method involves wrapping it in a service that exposes one or more endpoints that can receive input (e.g., text) and return the model\u2019s predictions. This approach allows the model\u2019s functionality to be integrated seamlessly into various applications, including customer-facing web frontends, internal analytics software, and automated data processing pipelines."}),"\n",(0,s.jsx)("figure",{children:(0,s.jsx)("img",{src:"/img/APIChart.png",className:"image",alt:"API chart image",width:"70%"})}),"\n",(0,s.jsxs)(n.p,{children:["The primary benefit of deploying models behind API endpoints, rather than embedding them directly within applications, is ",(0,s.jsx)(n.strong,{children:"decoupling"}),". Specifically, deploying behind an API endpoint offers several key advantages:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Independent Scalability"}),": The model serving layer can scale independently, handling increased usage or traffic spikes without affecting the application\u2019s performance."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Standardized Interface"}),": An API provides a clear, consistent, and easily shareable interface, simplifying integration across diverse systems within an organization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simplified Dependency Management"}),": Rather than managing individual runtime environments and dependencies across multiple applications, teams can interact with the model through a centralized, standardized service."]}),"\n"]}),"\n",(0,s.jsx)("figure",{children:(0,s.jsx)("img",{src:"/img/benefitsofDeployingModel.png",className:"image",alt:"Deployment Benifits image",width:"70%"})}),"\n",(0,s.jsxs)(n.p,{children:["Studies have shown, however, that only a fraction of trained ML models ever reach production environments, highlighting the difficulty and complexity involved",(0,s.jsx)(n.sup,{children:(0,s.jsx)(n.a,{href:"#user-content-fn-4",id:"user-content-fnref-4","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"1"})}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"the-deployment-bottleneck",children:"The Deployment Bottleneck"}),"\n",(0,s.jsx)(n.p,{children:"As mentioned, deploying even seemingly simple NLP models into a production environment is notoriously complex and introduces a distinct set of challenges that align more closely with software engineering and operations (DevOps/MLOps) than data science. Model deployment shares the same concerns as deploying any software service."}),"\n",(0,s.jsx)(n.p,{children:"Teams, especially those without dedicated MLOps or DevOps specialists, often face significant technical hurdles, these include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Infrastructure Setup"}),": Deploying models typically requires provisioning cloud infrastructure (compute instances, API gateways, storage buckets). For engineers unfamiliar with Infrastructure as Code (IaC) or cloud platforms, this might involve manual configuration, increasing risks of misconfigurations, insecurity, and inconsistency. Setting up access control, secrets management, and networking can also be overwhelming."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dependency and Environment Management"}),": NLP models rely on complex stacks of libraries (e.g., spaCy, numpy, transformers) with specific versions. Mismatches between development and production environments can cause silent failures or incorrect results. Dependency drift requires careful management, often using Docker containerization or virtual environments with pinned versions."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reproducibility"}),": Manual deployments are hard to reproduce or audit. Without declarative configurations (IaC frameworks like AWS CDK or Terraform), tracking the production state is difficult, making rollbacks or handoffs fragile."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring and Reliability"}),": A deployed model is a live service often requiring uptime and observability. Without monitoring (health checks, tracing, logging, metrics), teams may be unaware of failures, performance degradation, or data drift."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Security and Access Control"}),": Exposing models via APIs creates attack surfaces. Access control (API keys, rate limits, authentication) helps prevent unauthorized use or data exposure."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"API Development"}),": Building and maintaining the API interface itself requires software engineering effort."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scalability and Performance"}),": Ensuring the service can handle the required load and meet latency requirements."]}),"\n"]}),"\n",(0,s.jsx)("figure",{children:(0,s.jsx)("img",{src:"/img/deploymentChallenges.png",className:"image",alt:"Deployment Challanges image",width:"70%"})}),"\n",(0,s.jsx)(n.p,{children:"Many deployment workflows remain fragile and sensitive to inconsistencies between development and production. This is especially problematic in teams without dedicated expertise, where deployment becomes a bottleneck."}),"\n",(0,s.jsx)(n.h2,{id:"roles-and-responsibilities",children:"Roles and Responsibilities"}),"\n",(0,s.jsx)("figure",{children:(0,s.jsx)("img",{src:"/img/deploymentGap_fixedCliff.png",className:"image",alt:"image",width:"85%"})}),"\n",(0,s.jsx)(n.p,{children:"Successfully navigating the ML lifecycle typically involves distinct roles:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Scientist:"})," Focuses on understanding data, selecting/developing algorithms, training models, and evaluating their performance. Their primary output is often a trained model artifact."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ML Engineer / DevOps / Platform Engineer:"})," Focuses on the operational aspects \u2013 taking the trained model artifact and building a robust, scalable, and reliable infrastructure to serve in production. They are responsible for packaging, managing deployment pipelines (CI/CD), automating infrastructure, monitoring, and scaling."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The challenge arises in organizations, particularly smaller teams or those new to ML, where the specialized ML Engineer role may not exist. In such cases, the deployment responsibility often falls to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Scientists:"})," Who may lack the necessary software engineering, cloud infrastructure, or DevOps expertise and time."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Generalist Software Engineers:"})," Who may lack specific MLOps knowledge and tooling experience."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:'This "deployment gap" means that valuable, trained models often remain stuck, relegated to local machines and becoming part of the "model graveyard" instead of delivering value.'}),"\n",(0,s.jsx)(n.h2,{id:"deployment-strategies",children:"Deployment Strategies"}),"\n",(0,s.jsx)(n.p,{children:"Understanding the deployment strategy is essential when designing a solution, as different approaches have significant implications for infrastructure, latency, and operational complexity. NLP model deployments generally fall into one of two categories."}),"\n",(0,s.jsx)("figure",{children:(0,s.jsx)("img",{src:"/img/realtimeVsBatchDeploy.png",className:"image",alt:"Realtime vs batch image",width:"70%"})}),"\n",(0,s.jsx)(n.h3,{id:"real-time-deployment",children:"Real-Time Deployment"}),"\n",(0,s.jsx)(n.p,{children:"Real-time deployment exposes models through an always-available API endpoint that returns predictions immediately in response to input. This is essential when user experience depends on rapid model inference."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use Cases:"})," Chatbots, live sentiment scoring, email routing, and interactive applications."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deployment Characteristics:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Requires low-latency inference"}),"\n",(0,s.jsx)(n.li,{children:"Needs to handle concurrent requests"}),"\n",(0,s.jsx)(n.li,{children:"Typically integrated directly into user-facing systems"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Engineering Considerations:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Cold starts and performance tuning often must be addressed"}),"\n",(0,s.jsx)(n.li,{children:"Robust monitoring and alerting is frequently required"}),"\n",(0,s.jsx)(n.li,{children:"Typically paired with caching or queuing systems to handle bursts in traffic"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"While real-time deployments unlock broader use cases, they introduce additional complexity. High-throughput use cases may require autoscaling container orchestration platforms such as ECS or Kubernetes, architectures that are more difficult for small teams to manage."}),"\n",(0,s.jsx)(n.h3,{id:"batch-deployment",children:"Batch Deployment"}),"\n",(0,s.jsx)(n.p,{children:"Batch deployment refers to running models on a fixed schedule or over larger datasets at once, typically as part of an offline processing pipeline."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use Cases:"})," Processing historical data, extracting insights from a corpus of documents, sentiment analysis on daily customer reviews, etc."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deployment Characteristics:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Invocation frequency is low"}),"\n",(0,s.jsx)(n.li,{children:"Longer execution durations are tolerated"}),"\n",(0,s.jsx)(n.li,{children:"Outputs are often not returned in real-time, but rather are stored in a database or file system"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Engineering Considerations:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Lower focus on latency"}),"\n",(0,s.jsx)(n.li,{children:"Simpler to break up tasks to run at the same time"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"For Nimbus, batch processing aligns closely with the needs of our target users\u2014small data science teams or individual developers building internal tools to analyze unstructured data such as customer feedback, product reviews, or team recommendations. These models are not invoked constantly or in high-throughput production environments, but rather are executed intermittently as part of internal workflows. Supporting batch-style deployments allows Nimbus to optimize for low cost, low complexity, and quick deployment turnaround."}),"\n",(0,s.jsx)(n.h2,{id:"existing-deployment-solutions",children:"Existing Deployment Solutions"}),"\n",(0,s.jsx)(n.p,{children:"Given the aforementioned challenges and the choice between batch and real-time strategies, teams usually rely on external deployment solutions, each with its own strengths and drawbacks. Existing options generally fall into three categories:"}),"\n",(0,s.jsx)(n.h3,{id:"managed-platforms",children:"Managed Platforms"}),"\n",(0,s.jsx)("figure",{children:(0,s.jsx)("img",{src:"/img/SageMakerComic.png",className:"image",alt:"Sage maker image",width:"70%"})}),"\n",(0,s.jsxs)(n.p,{children:["Managed platforms like ",(0,s.jsx)(n.strong,{children:"AWS SageMaker"})," simplify deployment significantly by providing ",(0,s.jsx)(n.em,{children:"pre-configured infrastructure"})," and ",(0,s.jsx)(n.em,{children:"built-in scalability"}),". They handle many underlying complexities and offer convenient tools for monitoring and managing models. These services, however, are often expensive, particularly for smaller or budget-conscious teams, as costs can quickly accumulate, especially with continuous or idle usage. Additionally, managed services may result in vendor lock-in, restricting flexibility and control over infrastructure and data privacy. As a result, these platforms, while powerful and user-friendly, often don\u2019t align well with smaller teams\u2019 needs or budgets."]}),"\n",(0,s.jsx)(n.h3,{id:"open-source-model-serving-frameworks",children:"Open-Source Model-Serving Frameworks"}),"\n",(0,s.jsx)("figure",{children:(0,s.jsx)("img",{src:"/img/BentoMLComic.png",className:"image",alt:"Bento image",width:"70%"})}),"\n",(0,s.jsx)(n.p,{children:"Open-source model-serving frameworks like BentoML offer greater flexibility and customizability than managed platforms, enabling teams to tailor deployments closely to their specific requirements. This flexibility, however, often comes at a steep cost in terms of configuration complexity and required expertise. These frameworks typically demand extensive initial configuration and ongoing management, making them challenging for smaller teams that lack dedicated MLOps or DevOps personnel. While these tools can support diverse model types and deployment patterns, the corresponding operational overhead and configuration frequently outweigh their benefits, especially for simpler, focused NLP deployments."}),"\n",(0,s.jsx)(n.h3,{id:"diy-solutions",children:"DIY Solutions"}),"\n",(0,s.jsx)("figure",{children:(0,s.jsx)("img",{src:"/img/DockerComic.png",className:"image",alt:"DIY image",width:"70%"})}),"\n",(0,s.jsx)(n.p,{children:"The 'do-it-yourself' approach, using tools like Flask/FastAPI and Docker, offers more control but comes at a high cost in complexity and effort. Teams choosing this path must manually build and maintain the entire infrastructure stack needed to serve their model, which is a significant and often underestimated task. This involves constructing everything from the ground up, a process that is not only complex and time-consuming but also difficult to standardize; this can lead to inconsistencies and errors."}),"\n",(0,s.jsx)(n.h2,{id:"current-deployment-gap",children:"Current Deployment Gap"}),"\n",(0,s.jsx)(n.p,{children:"ly to fill this deployment gap by providing an automated, simple, and cost-effective pathway to get lightweight NLP models (currently SpaCy models for batch processing) into production quickly and reliably, without requiring deep MLOps expertise."}),"\n","\n",(0,s.jsxs)(n.section,{"data-footnotes":!0,className:"footnotes",children:[(0,s.jsx)(n.h2,{className:"sr-only",id:"footnote-label",children:"Footnotes"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{id:"user-content-fn-4",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://www.gartner.com/en/newsroom/press-releases/2020-10-19-gartner-identifies-the-top-strategic-technology-trends-for-2021",children:"https://www.gartner.com/en/newsroom/press-releases/2020-10-19-gartner-identifies-the-top-strategic-technology-trends-for-2021"}),"; ",(0,s.jsx)(n.a,{href:"https://www.forbes.com/councils/forbestechcouncil/2023/04/10/why-most-machine-learning-applications-fail-to-deploy/",children:"https://www.forbes.com/councils/forbestechcouncil/2023/04/10/why-most-machine-learning-applications-fail-to-deploy/"}),"; ",(0,s.jsx)(n.a,{href:"https://d2iq.com/blog/why-87-of-ai-ml-projects-never-make-it-into-production-and-how-to-fix-it",children:"https://d2iq.com/blog/why-87-of-ai-ml-projects-never-make-it-into-production-and-how-to-fix-it"})," ",(0,s.jsx)(n.a,{href:"#user-content-fnref-4","data-footnote-backref":"","aria-label":"Back to reference 1",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var t=i(6540);const s={},r=t.createContext(s);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);