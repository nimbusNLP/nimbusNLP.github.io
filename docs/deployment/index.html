<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-deployment" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.3">
<title data-rh="true">Deployment: The Bridge to Value | Nimbus</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://nimbusNLP.github.io/img/nimbusMain.png"><meta data-rh="true" name="twitter:image" content="https://nimbusNLP.github.io/img/nimbusMain.png"><meta data-rh="true" property="og:url" content="https://nimbusNLP.github.io/docs/deployment"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Deployment: The Bridge to Value | Nimbus"><meta data-rh="true" name="description" content="In order to utilize these models, they must be deployed or put into production. There are multiple ways to deploy a model, but a common method involves wrapping it in a service that exposes one or more endpoints that can receive input (e.g., text) and return the model’s predictions. This approach allows the model’s functionality to be integrated seamlessly into various applications such as customer-facing web frontends, internal analytics software, or automated data processing pipelines."><meta data-rh="true" property="og:description" content="In order to utilize these models, they must be deployed or put into production. There are multiple ways to deploy a model, but a common method involves wrapping it in a service that exposes one or more endpoints that can receive input (e.g., text) and return the model’s predictions. This approach allows the model’s functionality to be integrated seamlessly into various applications such as customer-facing web frontends, internal analytics software, or automated data processing pipelines."><link data-rh="true" rel="icon" href="/img/nimbusMain.png"><link data-rh="true" rel="canonical" href="https://nimbusNLP.github.io/docs/deployment"><link data-rh="true" rel="alternate" href="https://nimbusNLP.github.io/docs/deployment" hreflang="en"><link data-rh="true" rel="alternate" href="https://nimbusNLP.github.io/docs/deployment" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.054fdd1d.css">
<script src="/assets/js/runtime~main.698302b0.js" defer="defer"></script>
<script src="/assets/js/main.50fc280c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/nimbusMain.png" alt="Nimbus Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/nimbusMain.png" alt="Nimbus Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Nimbus</b></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs/introduction">CASE STUDY</a><a class="navbar__item navbar__link" href="/#team">TEAM</a><a href="https://github.com/nimbusNLP" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GITHUB<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/introduction">1. Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/background">2. Background</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/docs/deployment">3. Deployment - The Bridge to Value</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/architecture">4. architecture</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/tradeoffs">5. Trade-offs &amp; Challenges</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/futureWorkd">6. Future Work</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">3. Deployment - The Bridge to Value</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Deployment: The Bridge to Value</h1></header>
<p>In order to utilize these models, they must be deployed or put into production. There are multiple ways to deploy a model, but a common method involves wrapping it in a service that exposes one or more endpoints that can receive input (e.g., text) and return the model’s predictions. This approach allows the model’s functionality to be integrated seamlessly into various applications such as customer-facing web frontends, internal analytics software, or automated data processing pipelines.</p>
<p>![][image8]</p>
<p>The primary benefit of deploying models behind API endpoints, rather than embedding them directly within applications, is <strong>decoupling</strong>. Specifically, deploying behind an API endpoint offers several key advantages:</p>
<ul>
<li><strong>Independent Scalability</strong>: The model serving layer can scale independently, handling increased usage or traffic spikes without affecting the application’s performance.</li>
<li><strong>Standardized Interface</strong>: An API provides a clear, consistent, and easily shareable interface, simplifying integration across diverse systems within an organization</li>
<li><strong>Simplified Dependency Management</strong>: Rather than managing individual runtime environments and dependencies across multiple applications, teams can interact with the model through a centralized, standardized service.</li>
</ul>
<p>![][image9]</p>
<p>Building an NLP model is only half the challenge. For a model to deliver real value, it must be deployed into production environments where internal teams, external applications, or end-users can directly access its capabilities. Studies have shown, however, that only a fraction of trained ML models ever reach production environments, highlighting the difficulty and complexity involved[^4].</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-deployment-bottleneck-challenges-complexities-and-the-skill-gap">The Deployment Bottleneck: Challenges, Complexities, and the Skill Gap<a href="#the-deployment-bottleneck-challenges-complexities-and-the-skill-gap" class="hash-link" aria-label="Direct link to The Deployment Bottleneck: Challenges, Complexities, and the Skill Gap" title="Direct link to The Deployment Bottleneck: Challenges, Complexities, and the Skill Gap">​</a></h2>
<p>Deploying even seemingly simple NLP models into a production environment is notoriously complex and introduces a distinct set of challenges that align more closely with software engineering and operations (DevOps/MLOps) than data science. Model deployment shares the same concerns as deploying any software service.</p>
<p>Teams, especially those without dedicated MLOps or DevOps specialists, often face significant technical hurdles. These include:</p>
<ul>
<li><strong>Infrastructure Setup</strong>: Deploying models typically requires provisioning cloud infrastructure (compute instances, API gateways, storage buckets). For engineers unfamiliar with Infrastructure as Code (IaC) or cloud platforms, this might involve manual configuration, increasing risks of misconfigurations, insecurity, and inconsistency. Setting up access control, secrets management, and networking can also be overwhelming.</li>
<li><strong>Dependency and Environment Management</strong>: NLP models rely on complex stacks of libraries (e.g., spaCy, numpy, transformers) with specific versions. Mismatches between development and production environments can cause silent failures or incorrect results. Dependency drift requires careful management, often using Docker containerization or virtual environments with pinned versions.</li>
<li><strong>Reproducibility</strong>: Manual deployments are hard to reproduce or audit. Without declarative configurations (IaC frameworks like AWS CDK or Terraform), tracking the production state is difficult, making rollbacks or handoffs fragile.</li>
<li><strong>Monitoring and Reliability</strong>: A deployed model is a live service often requiring uptime and observability. Without monitoring (health checks, tracing, logging, metrics), teams may be unaware of failures, performance degradation, or data drift.</li>
<li><strong>Security and Access Control</strong>: Exposing models via APIs creates attack surfaces. Access control (API keys, rate limits, authentication) helps prevent unauthorized use or data exposure.</li>
<li><strong>API Development</strong>: Building and maintaining the API interface itself requires software engineering effort.</li>
<li><strong>Scalability and Performance</strong>: Ensuring the service can handle the required load and meet latency requirements.</li>
</ul>
<p>Many deployment workflows remain fragile and sensitive to inconsistencies between development and production. This fragility is especially problematic in teams without dedicated expertise, where deployment becomes a bottleneck.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bridging-the-gap-roles-and-responsibilities">Bridging the Gap: Roles and Responsibilities<a href="#bridging-the-gap-roles-and-responsibilities" class="hash-link" aria-label="Direct link to Bridging the Gap: Roles and Responsibilities" title="Direct link to Bridging the Gap: Roles and Responsibilities">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="image10">![][image10]<a href="#image10" class="hash-link" aria-label="Direct link to ![][image10]" title="Direct link to ![][image10]">​</a></h3>
<p>Successfully navigating the ML lifecycle typically involves distinct roles:</p>
<ul>
<li><strong>Data Scientist:</strong> Focuses on understanding data, selecting/developing algorithms, training models, and evaluating their performance. Their primary output is often a trained model artifact.</li>
<li><strong>ML Engineer / DevOps / Platform Engineer:</strong> Focuses on the operational aspects – taking the trained model artifact and building a robust, scalable, and reliable infrastructure to serve in production. They are responsible for packaging, managing deployment pipelines (CI/CD), automating infrastructure, monitoring, and scaling.</li>
</ul>
<p>The challenge arises in organizations, particularly smaller teams or those new to ML, where the specialized ML Engineer role may not exist. In such cases, the deployment responsibility often falls to:</p>
<ul>
<li><strong>Data Scientists:</strong> Who may lack the necessary software engineering, cloud infrastructure, or DevOps expertise and time.</li>
<li><strong>Generalist Software Engineers:</strong> Who may lack specific MLOps knowledge and tooling experience.</li>
</ul>
<p>This &quot;deployment gap&quot; means that valuable, trained models often remain stuck, relegated to local machines and becoming part of the &quot;model graveyard&quot; instead of delivering value.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deployment-strategies-batch-vs-real-time">Deployment Strategies: Batch vs. Real-Time<a href="#deployment-strategies-batch-vs-real-time" class="hash-link" aria-label="Direct link to Deployment Strategies: Batch vs. Real-Time" title="Direct link to Deployment Strategies: Batch vs. Real-Time">​</a></h2>
<p>![][image11]</p>
<p>Understanding the deployment strategy is essential when designing a solution, as different approaches have significant implications for infrastructure, latency, and operational complexity. NLP model deployments generally fall into one of two categories.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="batch-deployment">Batch Deployment<a href="#batch-deployment" class="hash-link" aria-label="Direct link to Batch Deployment" title="Direct link to Batch Deployment">​</a></h3>
<p>Batch deployment refers to running models on a fixed schedule or over larger datasets at once, typically as part of an offline processing pipeline.</p>
<ul>
<li><strong>Use Cases:</strong> Processing historical data, extracting insights from a corpus of documents, sentiment analysis on daily customer reviews, etc.</li>
<li><strong>Deployment Characteristics:</strong>
<ul>
<li>Low-frequency invocation</li>
<li>Tolerates longer execution durations</li>
<li>Outputs are typically stored in a database or file system, not returned in real-time</li>
</ul>
</li>
<li><strong>Engineering Considerations:</strong>
<ul>
<li>Less focus on endpoint latency</li>
<li>Easier to parallelize and scale horizontally</li>
</ul>
</li>
</ul>
<p>For Nimbus, batch processing aligns closely with the needs of our target users—small data science teams or individual developers building internal tools to analyze unstructured data like customer feedback, product reviews, or team recommendations. These models are not invoked constantly or in high-throughput production environments, but rather are executed intermittently as part of internal workflows. Supporting batch-style deployments allows Nimbus to optimize for low cost, low complexity, and quick deployment turnaround.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="real-time-deployment">Real-Time Deployment<a href="#real-time-deployment" class="hash-link" aria-label="Direct link to Real-Time Deployment" title="Direct link to Real-Time Deployment">​</a></h3>
<p>Real-time deployment exposes models through an always-available API endpoint that returns predictions immediately in response to input. This is essential when user experience depends on rapid model inference.</p>
<ul>
<li><strong>Use Cases:</strong> Chatbots, live sentiment scoring, email routing, and interactive applications.</li>
<li><strong>Deployment Characteristics:</strong>
<ul>
<li>Requires low-latency inference</li>
<li>Needs to handle concurrent requests</li>
<li>Typically integrated directly into user-facing systems</li>
</ul>
</li>
<li><strong>Engineering Considerations:</strong>
<ul>
<li>Must address cold starts and performance tuning</li>
<li>Requires robust monitoring and alerting</li>
<li>Often paired with caching or queuing systems to handle bursts in traffic</li>
</ul>
</li>
</ul>
<p>While real-time deployments unlock broader use cases, they introduce additional complexity. For example, cold-start latency becomes a significant concern when using serverless functions like AWS Lambda. Additionally, high-throughput use cases may require autoscaling container orchestration platforms such as ECS or Kubernetes, architectures that are more difficult for small teams to manage.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="navigating-the-landscape-existing-deployment-solutions">Navigating the Landscape: Existing Deployment Solutions<a href="#navigating-the-landscape-existing-deployment-solutions" class="hash-link" aria-label="Direct link to Navigating the Landscape: Existing Deployment Solutions" title="Direct link to Navigating the Landscape: Existing Deployment Solutions">​</a></h2>
<p>Given the aforementioned challenges and the choice between batch and real-time strategies, teams usually rely on external deployment solutions, each with its own strengths and drawbacks. Existing options generally fall into three categories:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="managed-platforms-eg-aws-sagemaker">Managed Platforms (e.g., AWS SageMaker)<a href="#managed-platforms-eg-aws-sagemaker" class="hash-link" aria-label="Direct link to Managed Platforms (e.g., AWS SageMaker)" title="Direct link to Managed Platforms (e.g., AWS SageMaker)">​</a></h3>
<p>![][image12]</p>
<p>Managed platforms like <strong>AWS SageMaker</strong> simplify deployment significantly by providing <em>pre-configured infrastructure</em> and <em>built-in scalability</em>. They handle many underlying complexities and offer convenient tools for monitoring and managing models. These services, however, are often expensive, particularly for smaller or budget-conscious teams, as costs can quickly accumulate, especially with continuous or idle usage. Additionally, managed services may result in vendor lock-in, restricting flexibility and control over infrastructure and data privacy. As a result, these platforms, while powerful and user-friendly, often don’t align well with smaller teams’ needs or budgets.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="open-source-model-serving-frameworks-eg-bentoml">Open-Source Model-Serving Frameworks (e.g., BentoML)<a href="#open-source-model-serving-frameworks-eg-bentoml" class="hash-link" aria-label="Direct link to Open-Source Model-Serving Frameworks (e.g., BentoML)" title="Direct link to Open-Source Model-Serving Frameworks (e.g., BentoML)">​</a></h3>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="image13">![][image13]<a href="#image13" class="hash-link" aria-label="Direct link to ![][image13]" title="Direct link to ![][image13]">​</a></h2>
<p>Open-source model-serving frameworks like BentoML offer greater flexibility and customizability than managed platforms, enabling teams to tailor deployments closely to their specific requirements. This flexibility, however, often comes at a steep cost in terms of configuration complexity and required expertise. These frameworks typically demand extensive initial configuration and ongoing management, making them challenging for smaller teams that lack dedicated MLOps or DevOps personnel. While these tools can support diverse model types and deployment patterns, the corresponding operational overhead and configuration frequently outweigh their benefits, especially for simpler, focused NLP deployments.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="diy-solutions-flask-fastapi-docker">DIY Solutions (Flask, FastAPI, Docker)<a href="#diy-solutions-flask-fastapi-docker" class="hash-link" aria-label="Direct link to DIY Solutions (Flask, FastAPI, Docker)" title="Direct link to DIY Solutions (Flask, FastAPI, Docker)">​</a></h3>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="image14">![][image14]<a href="#image14" class="hash-link" aria-label="Direct link to ![][image14]" title="Direct link to ![][image14]">​</a></h2>
<p>The &#x27;do-it-yourself&#x27; approach, using tools like Flask/FastAPI and Docker, offers more control but comes at a high cost in complexity and effort. Teams choosing this path must manually build and maintain the entire infrastructure stack needed to serve their model, which is a significant and often underestimated task. This involves constructing everything from the ground up, a process that is not only complex and time-consuming but also difficult to standardize, which can lead to inconsistencies and errors.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-opportunity-the-deployment-gap-and-the-need-for-simpler-solutions">The Opportunity: The Deployment Gap and the Need for Simpler Solutions<a href="#the-opportunity-the-deployment-gap-and-the-need-for-simpler-solutions" class="hash-link" aria-label="Direct link to The Opportunity: The Deployment Gap and the Need for Simpler Solutions" title="Direct link to The Opportunity: The Deployment Gap and the Need for Simpler Solutions">​</a></h2>
<p>![][image15]</p>
<p>The complexities inherent in NLP model deployment, the associated skill gaps, and the tradeoffs accompanying existing solutions collectively create a clearly identified deployment gap. Small data science teams and individual researchers often find themselves forced to choose between expensive managed services, overly complex open-source tools, or labor-and-expertise-intensive DIY setups.</p>
<p>This highlights a clear need: simpler, more accessible ways to deploy NLP models, especially for lightweight, task-specific models. Many teams require solutions that abstract away the intricate details of cloud infrastructure, containerization, API development, and ongoing operations. The ideal solution, particularly for smaller teams and batch-oriented or intermittent use cases, would automate the MLOps tasks, enabling data scientists to focus on model optimization and developers to focus on creatively integrating those models into their applications.</p>
<p>This creates the perfect context for Nimbus: a tool designed specifically to fill this deployment gap by providing an automated, simple, and cost-effective pathway to get lightweight NLP models (currently SpaCy models for batch processing) into production quickly and reliably, without requiring deep MLOps expertise.</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/background"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">2. Background</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/architecture"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">4. architecture</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-deployment-bottleneck-challenges-complexities-and-the-skill-gap" class="table-of-contents__link toc-highlight">The Deployment Bottleneck: Challenges, Complexities, and the Skill Gap</a></li><li><a href="#bridging-the-gap-roles-and-responsibilities" class="table-of-contents__link toc-highlight">Bridging the Gap: Roles and Responsibilities</a><ul><li><a href="#image10" class="table-of-contents__link toc-highlight">![][image10]</a></li></ul></li><li><a href="#deployment-strategies-batch-vs-real-time" class="table-of-contents__link toc-highlight">Deployment Strategies: Batch vs. Real-Time</a><ul><li><a href="#batch-deployment" class="table-of-contents__link toc-highlight">Batch Deployment</a></li><li><a href="#real-time-deployment" class="table-of-contents__link toc-highlight">Real-Time Deployment</a></li></ul></li><li><a href="#navigating-the-landscape-existing-deployment-solutions" class="table-of-contents__link toc-highlight">Navigating the Landscape: Existing Deployment Solutions</a><ul><li><a href="#managed-platforms-eg-aws-sagemaker" class="table-of-contents__link toc-highlight">Managed Platforms (e.g., AWS SageMaker)</a></li><li><a href="#open-source-model-serving-frameworks-eg-bentoml" class="table-of-contents__link toc-highlight">Open-Source Model-Serving Frameworks (e.g., BentoML)</a></li></ul></li><li><a href="#image13" class="table-of-contents__link toc-highlight">![][image13]</a><ul><li><a href="#diy-solutions-flask-fastapi-docker" class="table-of-contents__link toc-highlight">DIY Solutions (Flask, FastAPI, Docker)</a></li></ul></li><li><a href="#image14" class="table-of-contents__link toc-highlight">![][image14]</a></li><li><a href="#the-opportunity-the-deployment-gap-and-the-need-for-simpler-solutions" class="table-of-contents__link toc-highlight">The Opportunity: The Deployment Gap and the Need for Simpler Solutions</a></li></ul></div></div></div></div></main></div></div></div></div>
</body>
</html>